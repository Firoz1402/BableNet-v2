{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 22:02:03.631433: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-31 22:02:03.631504: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-31 22:02:03.632836: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-31 22:02:03.640717: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-31 22:02:04.496849: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, MultiHeadAttention, LayerNormalization, Add, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "lines=pd.read_csv(\"../data/Hindi_English_Truncated_Corpus.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ted</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ted</td>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ted</td>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tides</td>\n",
       "      <td>The then Governor of Kashmir resisted transfer...</td>\n",
       "      <td>कश्मीर के तत्कालीन गवर्नर ने इस हस्तांतरण का व...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>In this lies the circumstances of people befor...</td>\n",
       "      <td>इसमें तुमसे पूर्व गुज़रे हुए लोगों के हालात हैं।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ted</td>\n",
       "      <td>And who are we to say, even, that they are wrong</td>\n",
       "      <td>और हम होते कौन हैं यह कहने भी वाले कि वे गलत हैं</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>“”Global Warming“” refer to warming caused in ...</td>\n",
       "      <td>ग्लोबल वॉर्मिंग से आशय हाल ही के दशकों में हुई...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tides</td>\n",
       "      <td>You may want your child to go to a school that...</td>\n",
       "      <td>हो सकता है कि आप चाहते हों कि आप का नऋर्नमेनटे...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tides</td>\n",
       "      <td>Please ensure that you use the appropriate form .</td>\n",
       "      <td>कृपया यह सुनिश्चित कर लें कि आप सही फॉर्म का प...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>Category: Religious Text</td>\n",
       "      <td>श्रेणी:धर्मग्रन्थ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>This period summarily is pepped up with devotion.</td>\n",
       "      <td>यह काल समग्रतः भक्ति भावना से ओतप्रोत काल है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ted</td>\n",
       "      <td>So there is some sort of justice</td>\n",
       "      <td>तो वहाँ न्याय है</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tides</td>\n",
       "      <td>The first two were found unreliable and the pr...</td>\n",
       "      <td>पहले दो को अविश्वसनीय मानकर बाकी पांच मुखबिरों...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tides</td>\n",
       "      <td>They had justified their educational policy of...</td>\n",
       "      <td>कम संख़्या वाले उच्च एवं मध्यम श्रेणी के लोगों...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>And now at present the naturecure, Ayurvedic a...</td>\n",
       "      <td>हाल में नेपाल के हस्पताल सामन्यतया आयुर्वेद, प...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>Parliament time frame is 5 years and this will...</td>\n",
       "      <td>लोकसभा की कार्यावधि 5 वर्ष है पर्ंतु इसे समय स...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tides</td>\n",
       "      <td>ii Register Courts , empowered to try causes f...</td>\n",
       "      <td>रजिस्टर न्यायालय जिन्हें न्यायाधीश द्वारा प्रा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>Extreme weather due to increased mortality; di...</td>\n",
       "      <td>बढ़ती हुई मौतों displacements और आर्थिक नुकसान...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source                                   english_sentence  \\\n",
       "0         ted  politicians do not have permission to do what ...   \n",
       "1         ted         I'd like to tell you about one such child,   \n",
       "2   indic2012  This percentage is even greater than the perce...   \n",
       "3         ted  what we really mean is that they're bad at not...   \n",
       "4   indic2012  .The ending portion of these Vedas is called U...   \n",
       "5       tides  The then Governor of Kashmir resisted transfer...   \n",
       "6   indic2012  In this lies the circumstances of people befor...   \n",
       "7         ted   And who are we to say, even, that they are wrong   \n",
       "8   indic2012  “”Global Warming“” refer to warming caused in ...   \n",
       "9       tides  You may want your child to go to a school that...   \n",
       "10      tides  Please ensure that you use the appropriate form .   \n",
       "11  indic2012                           Category: Religious Text   \n",
       "12  indic2012  This period summarily is pepped up with devotion.   \n",
       "13        ted                   So there is some sort of justice   \n",
       "14      tides  The first two were found unreliable and the pr...   \n",
       "15      tides  They had justified their educational policy of...   \n",
       "16  indic2012  And now at present the naturecure, Ayurvedic a...   \n",
       "17  indic2012  Parliament time frame is 5 years and this will...   \n",
       "18      tides  ii Register Courts , empowered to try causes f...   \n",
       "19  indic2012  Extreme weather due to increased mortality; di...   \n",
       "\n",
       "                                       hindi_sentence  \n",
       "0   राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "1   मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "2    यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3      हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4         इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  \n",
       "5   कश्मीर के तत्कालीन गवर्नर ने इस हस्तांतरण का व...  \n",
       "6    इसमें तुमसे पूर्व गुज़रे हुए लोगों के हालात हैं।  \n",
       "7    और हम होते कौन हैं यह कहने भी वाले कि वे गलत हैं  \n",
       "8   ग्लोबल वॉर्मिंग से आशय हाल ही के दशकों में हुई...  \n",
       "9   हो सकता है कि आप चाहते हों कि आप का नऋर्नमेनटे...  \n",
       "10  कृपया यह सुनिश्चित कर लें कि आप सही फॉर्म का प...  \n",
       "11                                  श्रेणी:धर्मग्रन्थ  \n",
       "12      यह काल समग्रतः भक्ति भावना से ओतप्रोत काल है।  \n",
       "13                                   तो वहाँ न्याय है  \n",
       "14  पहले दो को अविश्वसनीय मानकर बाकी पांच मुखबिरों...  \n",
       "15  कम संख़्या वाले उच्च एवं मध्यम श्रेणी के लोगों...  \n",
       "16  हाल में नेपाल के हस्पताल सामन्यतया आयुर्वेद, प...  \n",
       "17  लोकसभा की कार्यावधि 5 वर्ष है पर्ंतु इसे समय स...  \n",
       "18  रजिस्टर न्यायालय जिन्हें न्यायाधीश द्वारा प्रा...  \n",
       "19  बढ़ती हुई मौतों displacements और आर्थिक नुकसान...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source              0\n",
       "english_sentence    2\n",
       "hindi_sentence      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(lines).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=lines[~pd.isnull(lines['english_sentence'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines=lines.sample(n=25000,random_state=42)\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all characters\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: x.lower())\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove quotes\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: re.sub(\"'\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "# Remove all the special characters\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all numbers from text\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
    "\n",
    "# Remove extra spaces\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: x.strip())\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.strip())\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start and end tokens to target sequences\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eng_words=set()\n",
    "for eng in lines['english_sentence']:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "all_hindi_words=set()\n",
    "for hin in lines['hindi_sentence']:\n",
    "    for word in hin.split():\n",
    "        if word not in all_hindi_words:\n",
    "            all_hindi_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30899"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_eng_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36900"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_hindi_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines['length_eng_sentence']=lines['english_sentence'].apply(lambda x:len(x.split(\" \")))\n",
    "lines['length_hin_sentence']=lines['hindi_sentence'].apply(lambda x:len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>length_eng_sentence</th>\n",
       "      <th>length_hin_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25520</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>islam is word from arabic and it full word is ...</td>\n",
       "      <td>START_ इस्लाम शब्द अरबी भाषा का शब्द है जिसका ...</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118633</th>\n",
       "      <td>ted</td>\n",
       "      <td>everything is reliant on these computers working</td>\n",
       "      <td>START_ इन कंप्यूटरों पर सब कुछ निर्भर है _END</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113495</th>\n",
       "      <td>tides</td>\n",
       "      <td>parliament does not control the government</td>\n",
       "      <td>START_ संसद का सरकार पपर नियंत्रण नपहीं रहता _END</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29783</th>\n",
       "      <td>tides</td>\n",
       "      <td>race equality new laws</td>\n",
       "      <td>START_ नये कानून नस्ली समानता _END</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111804</th>\n",
       "      <td>tides</td>\n",
       "      <td>the provision would not affect the power of pa...</td>\n",
       "      <td>START_ व्यवसायों आदि से होने वाली आय के बारे म...</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           source                                   english_sentence  \\\n",
       "25520   indic2012  islam is word from arabic and it full word is ...   \n",
       "118633        ted   everything is reliant on these computers working   \n",
       "113495      tides         parliament does not control the government   \n",
       "29783       tides                             race equality new laws   \n",
       "111804      tides  the provision would not affect the power of pa...   \n",
       "\n",
       "                                           hindi_sentence  \\\n",
       "25520   START_ इस्लाम शब्द अरबी भाषा का शब्द है जिसका ...   \n",
       "118633      START_ इन कंप्यूटरों पर सब कुछ निर्भर है _END   \n",
       "113495  START_ संसद का सरकार पपर नियंत्रण नपहीं रहता _END   \n",
       "29783                  START_ नये कानून नस्ली समानता _END   \n",
       "111804  START_ व्यवसायों आदि से होने वाली आय के बारे म...   \n",
       "\n",
       "        length_eng_sentence  length_hin_sentence  \n",
       "25520                    14                   21  \n",
       "118633                    7                    9  \n",
       "113495                    6                    9  \n",
       "29783                     4                    6  \n",
       "111804                   22                   24  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2463, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[lines['length_eng_sentence']>30].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=lines[lines['length_eng_sentence']<=20]\n",
    "lines=lines[lines['length_hin_sentence']<=20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum length of Hindi Sentence  20\n",
      "maximum length of English Sentence  20\n"
     ]
    }
   ],
   "source": [
    "print(\"maximum length of Hindi Sentence \",max(lines['length_hin_sentence']))\n",
    "print(\"maximum length of English Sentence \",max(lines['length_eng_sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_src=max(lines['length_hin_sentence'])\n",
    "max_length_tar=max(lines['length_eng_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(max_length_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30899, 36900)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_hindi_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_hindi_words)\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decoder_tokens += 1 #for zero padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>length_eng_sentence</th>\n",
       "      <th>length_hin_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15414</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>formation of the cabinet</td>\n",
       "      <td>START_ परिषद का गठन _END</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58870</th>\n",
       "      <td>tides</td>\n",
       "      <td>checking keep the methods you use to check as ...</td>\n",
       "      <td>START_ जांच करना ः जांच के तरीकों को जितना सम्...</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17652</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>she was awarded for literature by padma bhusha...</td>\n",
       "      <td>START_ में भारत सरकार ने उनकी साहित्यिक सेवा क...</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34538</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>single the decision of which can be challenged...</td>\n",
       "      <td>START_ एकल जिसके निर्णय को उच्च न्यायालय की डि...</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103268</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>by the name of “”hindi tool“” installer of kus...</td>\n",
       "      <td>START_ कुशीनारा का हिन्दीटूल्स नामक इंस्टालर ज...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10032</th>\n",
       "      <td>ted</td>\n",
       "      <td>with the commandments held by the leader</td>\n",
       "      <td>START_ एक नेता के दिये गये निर्देशों के अनुसार...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70519</th>\n",
       "      <td>tides</td>\n",
       "      <td>according to the findings of the trial court t...</td>\n",
       "      <td>START_ अदालत के निष्कर्षों में भगत सिंह के विर...</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28054</th>\n",
       "      <td>tides</td>\n",
       "      <td>these were the founders of the karnataka branc...</td>\n",
       "      <td>START_ ये ही कर्णाटक के कलचुरी कहलाए _END</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49071</th>\n",
       "      <td>ted</td>\n",
       "      <td>you can walk across to another group</td>\n",
       "      <td>START_ अगर आपको अपना समूह पसंद न आये तो आप _END</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65367</th>\n",
       "      <td>tides</td>\n",
       "      <td>but these hopes were not fulfilled</td>\n",
       "      <td>START_ किंतु ये आशायें पूर्ण नहीं हुई _END</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           source                                   english_sentence  \\\n",
       "15414   indic2012                           formation of the cabinet   \n",
       "58870       tides  checking keep the methods you use to check as ...   \n",
       "17652   indic2012  she was awarded for literature by padma bhusha...   \n",
       "34538   indic2012  single the decision of which can be challenged...   \n",
       "103268  indic2012  by the name of “”hindi tool“” installer of kus...   \n",
       "10032         ted           with the commandments held by the leader   \n",
       "70519       tides  according to the findings of the trial court t...   \n",
       "28054       tides  these were the founders of the karnataka branc...   \n",
       "49071         ted               you can walk across to another group   \n",
       "65367       tides                 but these hopes were not fulfilled   \n",
       "\n",
       "                                           hindi_sentence  \\\n",
       "15414                            START_ परिषद का गठन _END   \n",
       "58870   START_ जांच करना ः जांच के तरीकों को जितना सम्...   \n",
       "17652   START_ में भारत सरकार ने उनकी साहित्यिक सेवा क...   \n",
       "34538   START_ एकल जिसके निर्णय को उच्च न्यायालय की डि...   \n",
       "103268  START_ कुशीनारा का हिन्दीटूल्स नामक इंस्टालर ज...   \n",
       "10032   START_ एक नेता के दिये गये निर्देशों के अनुसार...   \n",
       "70519   START_ अदालत के निष्कर्षों में भगत सिंह के विर...   \n",
       "28054           START_ ये ही कर्णाटक के कलचुरी कहलाए _END   \n",
       "49071     START_ अगर आपको अपना समूह पसंद न आये तो आप _END   \n",
       "65367          START_ किंतु ये आशायें पूर्ण नहीं हुई _END   \n",
       "\n",
       "        length_eng_sentence  length_hin_sentence  \n",
       "15414                     4                    5  \n",
       "58870                    13                   17  \n",
       "17652                    13                   16  \n",
       "34538                    14                   17  \n",
       "103268                   12                   12  \n",
       "10032                     7                   11  \n",
       "70519                    20                   15  \n",
       "28054                    12                    8  \n",
       "49071                     7                   11  \n",
       "65367                     6                    8  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = shuffle(lines)\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13209,), (3303,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = lines['english_sentence'], lines['hindi_sentence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103865    maximum registered rainfall in a single day in...\n",
       "46880     the second book named “”the odecity of hope“” ...\n",
       "31524     controlled burn during spring cultivation in a...\n",
       "30434       the prevailing colour of these animals is white\n",
       "92018                       especially at “ zero interest ”\n",
       "                                ...                        \n",
       "30538          the white marble is used in its construction\n",
       "23274                                        is he up to it\n",
       "98138           akbars son jahangir built this mausoleum in\n",
       "81127     like by a protein being destroyed by proteolys...\n",
       "29978                        we can take away from cocacola\n",
       "Name: english_sentence, Length: 13209, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103865    START_ मुंबई में दर्ज एक दिन में सर्वोच्च वर्ष...\n",
       "46880     START_ उनकी दूसरी पुस्तक द ओडेसिटी आफ होप अक्त...\n",
       "31524     START_ दक्षिण जॉर्जिया के controlled burnएक खे...\n",
       "30434     START_ इस नस्ल के ढोरों का सामान्य रंग सफेद हो...\n",
       "92018                  START_ खासकर जब याज दर शून्य हो _END\n",
       "                                ...                        \n",
       "30538     START_ इसमें भी श्वेत संगमर्मर का प्रयोग हुआ ह...\n",
       "23274                START_ क्या वे इसके लिए तैयार हैं _END\n",
       "98138     START_ अकबर के पुत्र जहाँगीर ने इस मकबरे का नि...\n",
       "81127     START_ जैसे प्रोटीन प्रोटेओलाइसिस से ध्वस्त हो...\n",
       "29978            START_ जो हम कोकाकोला से सीख सकते हैं _END\n",
       "Name: hindi_sentence, Length: 13209, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(input_layer, head_size, num_heads, ff_dim, dropout=0):\n",
    "    attn_output = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(input_layer, input_layer)\n",
    "    attn_output = Dropout(dropout)(attn_output)\n",
    "    attn_output = Add()([input_layer, attn_output])\n",
    "    attn_output = LayerNormalization(epsilon=1e-6)(attn_output)\n",
    "    \n",
    "    ff_output = Dense(ff_dim, activation='relu')(attn_output)\n",
    "    ff_output = Dropout(dropout)(ff_output)\n",
    "    ff_output = Dense(input_layer.shape[-1])(ff_output)\n",
    "    ff_output = Add()([attn_output, ff_output])\n",
    "    ff_output = LayerNormalization(epsilon=1e-6)(ff_output)\n",
    "    \n",
    "    return ff_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_decoder(input_layer, encoder_output, head_size, num_heads, ff_dim, dropout=0):\n",
    "    attn_output_1 = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(input_layer, input_layer)\n",
    "    attn_output_1 = Dropout(dropout)(attn_output_1)\n",
    "    attn_output_1 = Add()([input_layer, attn_output_1])\n",
    "    attn_output_1 = LayerNormalization(epsilon=1e-6)(attn_output_1)\n",
    "    \n",
    "    attn_output_2 = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(attn_output_1, encoder_output)\n",
    "    attn_output_2 = Dropout(dropout)(attn_output_2)\n",
    "    attn_output_2 = Add()([attn_output_1, attn_output_2])\n",
    "    attn_output_2 = LayerNormalization(epsilon=1e-6)(attn_output_2)\n",
    "    \n",
    "    ff_output = Dense(ff_dim, activation='relu')(attn_output_2)\n",
    "    ff_output = Dropout(dropout)(ff_output)\n",
    "    ff_output = Dense(input_layer.shape[-1])(ff_output)\n",
    "    ff_output = Add()([attn_output_2, ff_output])\n",
    "    ff_output = LayerNormalization(epsilon=1e-6)(ff_output)\n",
    "    \n",
    "    return ff_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 22:02:06.872568: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-31 22:02:06.909155: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-31 22:02:06.909405: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-31 22:02:06.910974: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-31 22:02:06.911188: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-31 22:02:06.911370: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-31 22:02:06.979387: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-31 22:02:06.979607: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-31 22:02:06.979801: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-31 22:02:06.979952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2285 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "head_size = 256\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "num_encoder_tokens = len(all_eng_words)  # Replace with your actual vocabulary size\n",
    "num_decoder_tokens = len(all_hindi_words)  # Replace with your actual vocabulary size\n",
    "latent_dim = 256\n",
    "dropout = 0.1\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(num_encoder_tokens, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_outputs = transformer_encoder(enc_emb, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_outputs = transformer_decoder(dec_emb, encoder_outputs, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "# Final Dense Layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, None, 256)            7910144   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, None, 256)            1051904   ['embedding[0][0]',           \n",
      " iHeadAttention)                                                     'embedding[0][0]']           \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, None, 256)            0         ['multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " add (Add)                   (None, None, 256)            0         ['embedding[0][0]',           \n",
      "                                                                     'dropout[0][0]']             \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, None, 256)            512       ['add[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, None, 256)            9446400   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 512)            131584    ['layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, None, 256)            1051904   ['embedding_1[0][0]',         \n",
      " ltiHeadAttention)                                                   'embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, None, 512)            0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, None, 256)            0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, None, 256)            131328    ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, None, 256)            0         ['embedding_1[0][0]',         \n",
      "                                                                     'dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, None, 256)            0         ['layer_normalization[0][0]', \n",
      "                                                                     'dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, None, 256)            512       ['add_2[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, None, 256)            512       ['add_1[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, None, 256)            1051904   ['layer_normalization_2[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, None, 256)            0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, None, 256)            0         ['layer_normalization_2[0][0]'\n",
      "                                                                    , 'dropout_3[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, None, 256)            512       ['add_3[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, None, 512)            131584    ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, None, 512)            0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, None, 256)            131328    ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, None, 256)            0         ['layer_normalization_3[0][0]'\n",
      "                                                                    , 'dense_3[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, None, 256)            512       ['add_4[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, None, 36900)          9483300   ['layer_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 30523940 (116.44 MB)\n",
      "Trainable params: 30523940 (116.44 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 130\n",
    "epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15676/2713035456.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 22:02:10.161100: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-31 22:02:11.332403: I external/local_xla/xla/service/service.cc:168] XLA service 0x7ff72c07c750 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-31 22:02:11.332438: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2024-05-31 22:02:11.336217: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-31 22:02:11.346451: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717173131.399731   15815 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 57s 517ms/step - loss: 7.2937 - val_loss: 6.6359\n",
      "Epoch 2/60\n",
      "101/101 [==============================] - 51s 507ms/step - loss: 6.1363 - val_loss: 5.9681\n",
      "Epoch 3/60\n",
      "101/101 [==============================] - 51s 504ms/step - loss: 5.4545 - val_loss: 5.6511\n",
      "Epoch 4/60\n",
      "101/101 [==============================] - 51s 503ms/step - loss: 4.8581 - val_loss: 5.4022\n",
      "Epoch 5/60\n",
      "101/101 [==============================] - 51s 506ms/step - loss: 4.3049 - val_loss: 5.1101\n",
      "Epoch 6/60\n",
      "101/101 [==============================] - 51s 505ms/step - loss: 3.7906 - val_loss: 4.9630\n",
      "Epoch 7/60\n",
      "101/101 [==============================] - 51s 506ms/step - loss: 3.3142 - val_loss: 4.8323\n",
      "Epoch 8/60\n",
      "101/101 [==============================] - 51s 505ms/step - loss: 2.8652 - val_loss: 4.7249\n",
      "Epoch 9/60\n",
      "101/101 [==============================] - 51s 505ms/step - loss: 2.4560 - val_loss: 4.6224\n",
      "Epoch 10/60\n",
      "101/101 [==============================] - 51s 505ms/step - loss: 2.0774 - val_loss: 4.5642\n",
      "Epoch 11/60\n",
      "101/101 [==============================] - 52s 511ms/step - loss: 1.7356 - val_loss: 4.5577\n",
      "Epoch 12/60\n",
      "101/101 [==============================] - 65s 641ms/step - loss: 1.4358 - val_loss: 4.5786\n",
      "Epoch 13/60\n",
      "101/101 [==============================] - 81s 798ms/step - loss: 1.1599 - val_loss: 4.6072\n",
      "Epoch 14/60\n",
      "101/101 [==============================] - 79s 784ms/step - loss: 0.9368 - val_loss: 4.5997\n",
      "Epoch 15/60\n",
      " 17/101 [====>.........................] - ETA: 57s - loss: 0.8112"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples//batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_outputs)\n",
    "# Decoder setup for inference\n",
    "decoder_inputs_inf = Input(shape=(None,))\n",
    "dec_emb_inf = dec_emb_layer(decoder_inputs_inf)\n",
    "\n",
    "# The decoder needs the encoder output for cross-attention\n",
    "encoder_outputs_inf = Input(shape=(None, latent_dim))\n",
    "\n",
    "# Transformer Decoder layers for inference\n",
    "decoder_outputs_inf = transformer_decoder(dec_emb_inf, encoder_outputs_inf, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "# Final Dense layer for output generation\n",
    "decoder_outputs_inf = decoder_dense(decoder_outputs_inf)\n",
    "\n",
    "# Define the inference decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_inf, encoder_outputs_inf],\n",
    "    decoder_outputs_inf\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as context vectors (encoder output).\n",
    "    encoder_output_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    \n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        # Predict the next token\n",
    "        output_tokens = decoder_model.predict([target_seq, encoder_output_value])\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "        \n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "            len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_model.save('../models/english_to_hindi_translator/encoder_model_e2h_transformer.h5')\n",
    "decoder_model.save('../models/english_to_hindi_translator/decoder_model_e2h_transformer.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the tokenizers\n",
    "with open('../models/english_to_hindi_translator/english_tokenizer_e2h_transformer.pkl', 'wb') as f:\n",
    "    pickle.dump(input_token_index, f)\n",
    "\n",
    "with open('../models/english_to_hindi_translator/hindi_tokenizer_e2h_transformer.pkl', 'wb') as f:\n",
    "    pickle.dump(target_token_index, f)\n",
    "\n",
    "# Save the reverse tokenizers\n",
    "with open('../models/english_to_hindi_translator/reverse_english_tokenizer_e2h.pkl', 'wb') as f:\n",
    "    pickle.dump(reverse_input_char_index, f)\n",
    "\n",
    "with open('../models/english_to_hindi_translator/reverse_hindi_tokenizer_e2h.pkl', 'wb') as f:\n",
    "    pickle.dump(reverse_target_char_index, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Load the models\n",
    "encoder_model = load_model('../models/english_to_hindi_translator/encoder_model_e2h_transformer.h5')\n",
    "decoder_model = load_model('../models/english_to_hindi_translator/decoder_model_e2h_transformer.h5')\n",
    "\n",
    "# Load tokenizers and index mappings\n",
    "with open('../models/english_to_hindi_translator/english_tokenizer_e2h.pkl', 'rb') as f:\n",
    "    input_token_index = pickle.load(f)\n",
    "\n",
    "with open('../models/english_to_hindi_translator/hindi_tokenizer_e2h.pkl', 'rb') as f:\n",
    "    target_token_index = pickle.load(f)\n",
    "\n",
    "with open('../models/english_to_hindi_translator/reverse_english_tokenizer_e2h.pkl', 'rb') as f:\n",
    "    reverse_input_char_index = pickle.load(f)\n",
    "\n",
    "with open('../models/english_to_hindi_translator/reverse_hindi_tokenizer_e2h.pkl', 'rb') as f:\n",
    "    reverse_target_char_index = pickle.load(f)\n",
    "\n",
    "max_length_src = 20  \n",
    "latent_dim = 300\n",
    "num_decoder_tokens = len(target_token_index) + 1\n",
    "\n",
    "def translate_sentence(input_sentence):\n",
    "    input_seq = [input_token_index.get(word, 0) for word in input_sentence.split()]\n",
    "    input_seq = tf.keras.preprocessing.sequence.pad_sequences([input_seq], maxlen=max_length_src, padding='post')\n",
    "\n",
    "    # Encode the input as context vectors (encoder output)\n",
    "    encoder_output_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        # Predict the next token\n",
    "        output_tokens = decoder_model.predict([target_seq, encoder_output_value])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index.get(sampled_token_index, '')\n",
    "\n",
    "        # Exit condition: either hit max length or find stop character\n",
    "        if sampled_char == '_END' or len(decoded_sentence) > max_length_src:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence.append(sampled_char)\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "    return ' '.join(decoded_sentence)\n",
    "\n",
    "# Example usage\n",
    "print(translate_sentence(\"kya kar rha hai\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
